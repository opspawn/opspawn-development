# Debug Log: Task 7.2 - E2E Failure Test (`test_submit_task_and_expect_failure`) Investigation

**Date:** 2025-04-13 (Evening Session 4)

**Goal:** Investigate why `test_submit_task_and_expect_failure` fails by completing successfully, despite being configured with an invalid LLM provider intended to cause a failure.

**Initial State:**
*   The main DB visibility issue (stale `RUNNING` status) for successful tasks was resolved by adding an explicit commit in the worker (`_run_agent_task_logic`).
*   `test_submit_task_and_poll_completion` and `test_concurrent_task_submissions` pass.

**Debugging Steps & Findings:**

1.  **Run Test Suite:** Executed all live E2E tests (`pytest ops-core/tests/integration/test_live_e2e.py -m live -s`).
    *   **Result:** `test_submit_task_and_expect_failure` failed because the task completed successfully.
2.  **Fix `AttributeError` in Concurrent Test:** Corrected assertions in `test_concurrent_task_submissions` to use `final_task.result` instead of `final_task.output`.
3.  **Re-run Test Suite:** Executed tests again.
    *   **Result:** `test_concurrent_task_submissions` passed. `test_submit_task_and_expect_failure` still failed (completed successfully).
4.  **Implement Agent Config Handling:** Modified `_run_agent_task_logic` and `get_llm_client` in `ops-core/src/ops_core/scheduler/engine.py` to:
    *   Extract `llm_provider` override from `input_data["agent_config"]`.
    *   Pass the override to `get_llm_client`.
    *   Prioritize the override in `get_llm_client`.
    *   Raise a `ValueError` in `get_llm_client` if an *overridden* provider is unsupported.
    *   Catch the `ValueError` in `_run_agent_task_logic` and mark the task as `FAILED`, committing the result.
5.  **Re-run Test Suite:** Executed tests again.
    *   **Result:** `test_submit_task_and_expect_failure` still failed (completed successfully).
6.  **Analyze Logs:** Reviewed worker logs for the failing task (`task_82a0f311-dc2e-44f2-8232-b0a876e70fac`).
    *   Confirmed the `ValueError` for "non_existent_provider" was caught.
    *   Confirmed the worker logic proceeded to update status to `FAILED` and commit the transaction.
    *   Logs:
        ```
        [2025-04-13 21:06:28,424] [PID 289524] [Thread-2] [ops_core.scheduler.engine] [ERROR] Configuration error for task task_82a0f311-dc2e-44f2-8232-b0a876e70fac: Unsupported LLM provider specified in agent_config: non_existent_provider
        [2025-04-13 21:06:28,430] [PID 289524] [Thread-2] [ops_core.scheduler.engine] [INFO] VERBOSE_LOG: Task task_82a0f311-dc2e-44f2-8232-b0a876e70fac: Committed FAILED status and config error output.
        ```
7.  **Analyze Test Polling:** Reviewed test output.
    *   The API polling loop immediately retrieved `status: completed` for the task, causing the test assertion to fail.
8.  **Review API Endpoint & Schema:** Checked `get_task` in `api/v1/endpoints/tasks.py` and `TaskResponse` in `api/v1/schemas/tasks.py`.
    *   Endpoint uses `session.refresh()`.
    *   Schema includes `status` and `error_message`.
    *   No obvious reason found in API/Schema code for misreporting the status.
9.  **Fix Test Assertion:** Corrected assertions in `test_submit_task_and_expect_failure` to check `error_message` instead of `output`.
10. **Re-run Test Suite:** Executed tests again.
    *   **Result:** `test_submit_task_and_expect_failure` still failed (completed successfully). The test now correctly checks `error_message`, but the API polling still reports `completed` status initially.

**Conclusion:**
*   The worker correctly identifies the invalid configuration override, logs the error, updates the task status to `FAILED`, and commits the transaction.
*   Despite the worker committing `FAILED`, the API endpoint (`GET /tasks/{task_id}`) immediately reads the status as `COMPLETED` during the test's polling loop.
*   The reason for this discrepancy (worker commits FAILED, API reads COMPLETED) is unclear and persists even after fixing the main DB visibility issue for successful tasks and ensuring the API uses `session.refresh()`.

**Possible Causes / Next Debugging Steps (Paused):**
*   **Transaction Isolation:** Revisit transaction isolation levels, ensuring consistency between worker and API sessions/connections.
*   **Fixture Interaction:** Investigate potential interference from test fixtures managing DB sessions across processes.
*   **DB/Driver Behavior:** Explore potential edge cases in SQLAlchemy/asyncpg/PostgreSQL regarding transaction visibility.
*   **Direct DB Inspection:** Add breakpoints or logging to inspect the actual database row state immediately after the worker commit and during the API read within the test execution.
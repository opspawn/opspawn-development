from abc import ABC, abstractmethod
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional

class LlmResponse(BaseModel):
    """Standard response structure from an LLM call."""
    content: str = Field(..., description="The primary text content generated by the LLM.")
    model_used: Optional[str] = Field(None, description="Identifier of the specific model that generated the response.")
    usage_metadata: Optional[Dict[str, Any]] = Field(None, description="Token usage or other metadata returned by the API.")
    finish_reason: Optional[str] = Field(None, description="Reason the generation finished (e.g., 'stop', 'length').")
    error: Optional[str] = Field(None, description="Error message if the call failed.")

class BaseLlmClient(ABC):
    """Abstract base class for LLM client implementations."""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None, # Provider-specific default model if None
        stop_sequences: Optional[list[str]] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        timeout: Optional[float] = 60.0, # Default timeout in seconds
        **kwargs: Any # Allow provider-specific parameters
    ) -> LlmResponse:
        """
        Generates text based on a prompt using the configured LLM.

        Args:
            prompt: The input prompt for the LLM.
            model: The specific model identifier to use (optional, uses default if None).
            stop_sequences: List of sequences to stop generation at.
            temperature: Sampling temperature.
            max_tokens: Maximum number of tokens to generate.
            timeout: Optional request timeout in seconds (default: 60.0).
            **kwargs: Additional provider-specific arguments.

        Returns:
            An LlmResponse object containing the generated content and metadata.
        """
        pass
